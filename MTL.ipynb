{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d9c01a-2567-404f-a077-9e1a1b62d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import wandb\n",
    "from sklearn.metrics import recall_score, precision_score, confusion_matrix, top_k_accuracy_score\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add8587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMG_SIZE = 300 \n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 60\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9\n",
    "ALPHA = 1.0\n",
    "\n",
    "IMAGE_FOLDER = './data/image/yolo8x_300*300'\n",
    "TRAIN_CSV = 'final_train_80.csv'  \n",
    "VAL_CSV   = 'final_val_10.csv'    \n",
    "PUBTEST_CSV = './data/DanishFungi2024-Mini-pubtest.csv'\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8732c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class SafeShroomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.unique_classes = sorted(self.df['class_id'].unique())\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.unique_classes)}\n",
    "        self.num_classes = len(self.unique_classes)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.root_dir, os.path.basename(row['image_path']))\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            image = Image.new('RGB', (IMG_SIZE, IMG_SIZE))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        cls_idx = self.class_to_idx.get(row['class_id'], 0)\n",
    "        return image, cls_idx, float(row['poisonous'])\n",
    "\n",
    "class SafeShroomMTL(nn.Module):\n",
    "    def __init__(self, num_species):\n",
    "        super(SafeShroomMTL, self).__init__()\n",
    "        self.backbone = models.efficientnet_b3(weights='DEFAULT')\n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        self.species_head = nn.Linear(in_features, num_species)\n",
    "        self.tox_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 64), nn.ReLU(), nn.Linear(64, 1) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x)\n",
    "        return self.species_head(f), self.tox_head(f)\n",
    "\n",
    "class SafeShroomToxOnly(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SafeShroomToxOnly, self).__init__()\n",
    "        self.backbone = models.efficientnet_b3(weights='DEFAULT')\n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        self.tox_head = nn.Sequential(\n",
    "            nn.Linear(in_features, 64), nn.ReLU(), nn.Linear(64, 1) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x)\n",
    "        return self.tox_head(f)\n",
    "\n",
    "# Load Model\n",
    "def load_model(path, num_species):\n",
    "    checkpoint = torch.load(path, map_location=DEVICE)\n",
    "    \n",
    "    # Try MTL\n",
    "    try:\n",
    "        model = SafeShroomMTL(num_species)\n",
    "        model.load_state_dict(checkpoint)\n",
    "        return model, True # True = Is MTL\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    # Try Tox Only\n",
    "    try:\n",
    "        model = SafeShroomToxOnly()\n",
    "        model.load_state_dict(checkpoint)\n",
    "        return model, False # False = Not MTL\n",
    "    except:\n",
    "        print(f\"Error loading {path}\")\n",
    "        return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f861fe7e-7d1c-4504-abe6-23681c2f1e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)), \n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train\n",
    "def train():\n",
    "    wandb.init(project=\"SafeShroom\", name=\"SGD_MTL_Train\")\n",
    "    \n",
    "    # load data\n",
    "    train_data = SafeShroomDataset(TRAIN_CSV, IMAGE_FOLDER, train_transform)\n",
    "    val_data = SafeShroomDataset(VAL_CSV, IMAGE_FOLDER, val_transform)  \n",
    "    \n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=8)\n",
    "    val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=8)\n",
    "    \n",
    "    \n",
    "    print(f\"Stats: Train={len(train_data)} | Val={len(val_data)}\")\n",
    "    \n",
    "    # Set model\n",
    "    pos_weight = torch.tensor([15.0]).to(DEVICE)\n",
    "    model = SafeShroomMTL(train_data.num_classes).to(DEVICE)\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "    \n",
    "    criterion_sp = nn.CrossEntropyLoss()\n",
    "    criterion_tox = nn.BCEWithLogitsLoss(pos_weight=pos_weight) \n",
    "    \n",
    "    best_acc = 0\n",
    "    best_fnr = float('inf')\n",
    "    best_tox = 0\n",
    "    \n",
    "    # Train Loop\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        loop = tqdm(train_loader, desc=f\"Ep {epoch+1}\")\n",
    "        \n",
    "        for imgs, sp_lbl, tox_lbl in loop:\n",
    "            imgs, sp_lbl, tox_lbl = imgs.to(DEVICE), sp_lbl.to(DEVICE), tox_lbl.to(DEVICE).float().unsqueeze(1)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            sp_out, tox_out = model(imgs)\n",
    "            \n",
    "            loss_sp = criterion_sp(sp_out, sp_lbl)\n",
    "            loss_tox = criterion_tox(tox_out, tox_lbl)\n",
    "            total_loss = loss_sp + (ALPHA * loss_tox)\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            wandb.log({\"train_loss\": total_loss.item()})\n",
    "            loop.set_postfix(loss=total_loss.item())\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        sp_correct = 0\n",
    "        total = 0\n",
    "        all_tox_preds, all_tox_lbls = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imgs, sp_lbl, tox_lbl in val_loader:\n",
    "                imgs, sp_lbl, tox_lbl = imgs.to(DEVICE), sp_lbl.to(DEVICE), tox_lbl.to(DEVICE)\n",
    "                \n",
    "                sp_out, tox_out = model(imgs)\n",
    "                \n",
    "                # Loss for Scheduler\n",
    "                l_sp = criterion_sp(sp_out, sp_lbl)\n",
    "                l_tox = criterion_tox(tox_out, tox_lbl.float().unsqueeze(1))\n",
    "                val_loss += (l_sp + ALPHA * l_tox).item()\n",
    "                \n",
    "                # Species Metrics\n",
    "                _, sp_pred = torch.max(sp_out, 1)\n",
    "                sp_correct += (sp_pred == sp_lbl).sum().item()\n",
    "                total += sp_lbl.size(0)\n",
    "                \n",
    "                # Toxicity Metrics\n",
    "                tox_probs = torch.sigmoid(tox_out)\n",
    "                preds = (tox_probs > 0.5).float().cpu().numpy()\n",
    "                all_tox_preds.extend(preds)\n",
    "                all_tox_lbls.extend(tox_lbl.cpu().numpy())\n",
    "\n",
    "        # Calculate Metrics\n",
    "        sp_acc = 100 * sp_correct / total\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        tox_acc = 100 * (np.array(all_tox_preds) == np.array(all_tox_lbls)).sum() / len(all_tox_lbls)\n",
    "        tox_recall = recall_score(all_tox_lbls, all_tox_preds, pos_label=1, zero_division=0)\n",
    "        \n",
    "        cm = confusion_matrix(all_tox_lbls, all_tox_preds, labels=[0, 1])\n",
    "        fn = cm[1, 0]\n",
    "        actual_positives = cm[1, :].sum()\n",
    "        fnr = 100 * fn / (actual_positives + 1e-6)\n",
    "\n",
    "        print(f\"Ep {epoch+1} | Sp Acc: {sp_acc:.2f}% | Tox Recall: {tox_recall:.4f} | FNR: {fnr:.2f}%\")\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"epoch\": epoch+1,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"species_accuracy\": sp_acc,\n",
    "            \"toxicity_recall\": tox_recall,\n",
    "            \"toxicity_fnr\": fnr,\n",
    "            \"toxic_accuracy\": tox_acc,\n",
    "            \"lr\": optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "        \n",
    "        # Save Models\n",
    "        if sp_acc > best_acc:\n",
    "            best_acc = sp_acc\n",
    "            torch.save(model.state_dict(), \"sp_safeshroom_best.pth\")\n",
    "            print(f\"   Saved Best Species Model\")\n",
    "        if tox_acc > best_tox:\n",
    "            best_tox = tox_acc\n",
    "            torch.save(model.state_dict(), \"tox_safeshroom_best.pth\")\n",
    "            print(f\"   Saved Best Toxic Model\")\n",
    "        \n",
    "        if fnr < best_fnr:\n",
    "            best_fnr = fnr\n",
    "            torch.save(model.state_dict(), \"fnr_safeshroom_best.pth\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d82496f",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb275db8-1e16-4a91-9222-9675c7271763",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_LIST = [\n",
    "    \"./tox_safeshroom_best.pth\",\n",
    "    \"./tox_safeshroom_best_alpha2.0.pth\",\n",
    "    \"./tox_safeshroom_best_alpha5.0.pth\",\n",
    "    \"./tox_safeshroom_best_alpha10.0.pth\"\n",
    "    \n",
    "]\n",
    "REPORT_FILE = \"final_pubtest_report.txt\"\n",
    "\n",
    "# Inference\n",
    "def run_evaluation():\n",
    "    # Setup Transforms\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    train_dummy = SafeShroomDataset(TRAIN_CSV, IMAGE_FOLDER, transform=None)\n",
    "    \n",
    "    # Load test data\n",
    "    test_data = SafeShroomDataset(PUBTEST_CSV, IMAGE_FOLDER, test_transform, class_to_idx=train_dummy.class_to_idx)\n",
    "    test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "    # Prepare Report File\n",
    "    with open(REPORT_FILE, \"w\") as f:\n",
    "        f.write(\"Inference Report\\n\")\n",
    "        f.write(\"=========================\\n\\n\")\n",
    "\n",
    "    # 3. Iterate Models\n",
    "    for model_path in MODEL_LIST:\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"Skipping {model_path} (File not found)\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nProcessing: {model_path}\")\n",
    "        model, is_mtl = load_model(model_path, train_dummy.num_classes)\n",
    "        \n",
    "        if model is None: continue\n",
    "        model.to(DEVICE)\n",
    "        model.eval()\n",
    "        \n",
    "        all_sp_probs = []\n",
    "        all_sp_lbls = []\n",
    "        all_tox_preds = []\n",
    "        all_tox_lbls = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, sp_lbl, tox_lbl in tqdm(test_loader, desc=os.path.basename(model_path)):\n",
    "                imgs = imgs.to(DEVICE)\n",
    "                \n",
    "                if is_mtl:\n",
    "                    sp_out, tox_out = model(imgs)\n",
    "                    # Species\n",
    "                    sp_probs = torch.softmax(sp_out, dim=1)\n",
    "                    all_sp_probs.extend(sp_probs.cpu().numpy())\n",
    "                    all_sp_lbls.extend(sp_lbl.numpy())\n",
    "                else:\n",
    "                    tox_out = model(imgs)\n",
    "                \n",
    "                # Toxicity\n",
    "                tox_probs = torch.sigmoid(tox_out)\n",
    "                preds = (tox_probs > 0.5).float().cpu().numpy()\n",
    "                all_tox_preds.extend(preds)\n",
    "                all_tox_lbls.extend(tox_lbl.numpy())\n",
    "\n",
    "        # Calculate Metric\n",
    "        \n",
    "        # Toxicity\n",
    "        y_pred = np.array(all_tox_preds).flatten()\n",
    "        y_true = np.array(all_tox_lbls).flatten()\n",
    "        \n",
    "        final_tox_acc = 100 * (y_pred == y_true).sum() / len(y_true)\n",
    "        final_tox_recall = recall_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
    "        \n",
    "        cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "        fn = cm[1, 0] if cm.shape == (2,2) else 0\n",
    "        actual_positives = cm[1, :].sum() if cm.shape == (2,2) else 0\n",
    "        fnr = 100 * fn / (actual_positives + 1e-6)\n",
    "\n",
    "        # Species (Only if MTL)\n",
    "        if is_mtl:\n",
    "            valid_ids = np.arange(train_dummy.num_classes)\n",
    "            final_sp_top1 = top_k_accuracy_score(all_sp_lbls, all_sp_probs, k=1, labels=valid_ids) * 100\n",
    "            final_sp_top3 = top_k_accuracy_score(all_sp_lbls, all_sp_probs, k=3, labels=valid_ids) * 100\n",
    "            sp_top1_str = f\"{final_sp_top1:.2f}%\"\n",
    "            sp_top3_str = f\"{final_sp_top3:.2f}%\"\n",
    "        else:\n",
    "            sp_top1_str = \"N/A\"\n",
    "            sp_top3_str = \"N/A\"\n",
    "\n",
    "        # Output\n",
    "        output_str = (\n",
    "            f\"FINAL SLIDE RESULTS (PubTest)\\n\"\n",
    "            f\"Model: {os.path.basename(model_path)}\\n\"\n",
    "            f\"========================================\\n\"\n",
    "            f\"Species Top-1 Acc:  {sp_top1_str}\\n\"\n",
    "            f\"Species Top-3 Acc:  {sp_top3_str}\\n\"\n",
    "            f\"------------------------------\\n\"\n",
    "            f\"Toxicity Accuracy:  {final_tox_acc:.2f}%\\n\"\n",
    "            f\"Toxicity Recall:    {final_tox_recall:.4f}\\n\"\n",
    "            f\"FNR (Death Rate):   {fnr:.2f}%\\n\"\n",
    "            f\"Confusion Matrix:\\n{cm}\\n\"\n",
    "            f\"========================================\\n\"\n",
    "        )\n",
    "        \n",
    "        print(output_str)\n",
    "        \n",
    "        # Save to File\n",
    "        with open(REPORT_FILE, \"a\") as f:\n",
    "            f.write(output_str + \"\\n\")\n",
    "\n",
    "    print(f\"\\nReport saved to {REPORT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_evaluation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
